<!DOCTYPE html>
<html>
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <link href='https://fonts.googleapis.com/css?family=Chivo:900' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="../stylesheets/stylesheet.css" media="screen" />
    <link rel="stylesheet" type="text/css" href="../stylesheets/pygment_trac.css" media="screen" />
    <link rel="stylesheet" type="text/css" href="../stylesheets/print.css" media="print" />
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <script type="text/javascript"
      src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
    <title>Tapkee by lisitsyn</title>
  </head>

  <body>
    <div id="container">
      <div class="inner">

        <header>
          <h1>Tapkee</h1>
          <h2>An efficient dimension reduction toolbox</h2>
        </header>

        <hr>

        <section id="main_content">
        <h3>Locally Linear Embedding method</h3>
        Given a set of feature vectors \(X = \{ x_1, x_2, \dots x_N \} \) the Locally Linear Embedding algorithm proposes to perform the following steps:
        <ol>
          <li>Identify nearest neighbors. <br/>
          For each \( x \in X \) identify its \( k \) nearest neighbors, i.e. a set \( \mathcal{N}_x \) of \( k \) feature vectors such that
          \[ \arg\min_{\mathcal{N}_x} \sum_{x_n \in \mathcal{N}_x}\| x - x_n \|_2 \]
          <li>Compute linear reconstruction weights. <br/>
          For each \( x \in X \) compute weight vector \( w \in \mathbb{R}^n \) that minimizes
          \[ \| x - \sum_{i=1}^{k} w_i \mathcal{N}_x^{i} \|_2, ~~ \text{w.r.t.} ~~ \|w\|_2 = 1\]
          where \( \mathcal{N}_x^{i} \) is a \( i\)-th element of the set \( \mathcal{N}_x \). 
          The solution of the problem stated above can be found from the normalized solution of the following equation:
          \[ G w = 1_k, \]
          where \( G \) is a \( k \times k \) matrix such that \( G_{i,j} = (x - \mathcal{N}_x^{i})(x - \mathcal{N}_x^{j}) \) and \( 1_k \in \mathbb{R}^k \) is a vector of all ones.
          Obviously, the problem comes ill-posed in case \( k \) gets more than dimension of feature space \( X \). This can be avoided with the regularization:
          \[ G \leftarrow G + \varepsilon I, \]
          where \( \varepsilon \) is a pre-defined constant reconstruction shift (usually \( 10^{-3}\)).
          Once \( w \) is computed it is stored into the sparse alignment matrix \( L \) (initially set by zero) with the following equation:
          \[ L_{I,I} \leftarrow L_{I,I} + W, \]
          where \( I \) is a set containing indices of all element of the set \(\mathcal{N}_x\) and \( x \) itself, 
          \( L_{I,I} \) denotes all \( (i,j) \) elements of the sparse matrix L such that \( i,j \in I\) and \[ W = \begin{bmatrix} 1 & -w \\ -w^{T} & w^T w \end{bmatrix}\].
          </li>
          <li>Embedding through eigendecomposition. <br/>
          To obtain \( t \) features (coordinates) of embedded vectors solve the partial eigenproblem 
          \[ L f = \lambda f, \]
          for smallest eigenvalues \( \lambda_1, \dots, \lambda_t, \lambda_{t+1} \) and its corresponding eigenvectors \( f_1, \dots, f_t, f_{t+1} \). Drop the smallest eigenvalue
          \( \lambda_1 \sim 0 \) (with the corresponding eigenvector) and form embedding matrix such that \(i\)-th coordinate (\( i=1,\dots,N \)) of \(j\)-th eigenvector 
          (\( j=1,\dots,t\) ) corresponds to \(j\)-th coordinate of projected \(i\)-th vector.
          </li>
        </ol>
        </section>

        <footer>
          Tapkee is maintained by <a href="https://github.com/lisitsyn">lisitsyn</a><br>
          This page was generated by <a href="http://pages.github.com">GitHub Pages</a>. Tactile theme by <a href="http://twitter.com/jasonlong">Jason Long</a>.
        </footer>

        
      </div>
    </div>
  </body>
</html>
